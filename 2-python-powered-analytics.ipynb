{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89b55b2-41de-49ad-9655-b3a3fbc2fdcc",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "# Part 2: ...\n",
    "\n",
    "- They will need to create a stage for the permanent UDF\n",
    "- Re-do modin imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78585399-0c71-4363-9efe-c1bb3e0afa1b",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "## Python UDFs (Jason)\n",
    "\n",
    "Snowflake provides a variey of built-in functions like [`SUM()`](https://docs.snowflake.com/en/sql-reference/functions/sum), [`REGEXP_REPLACE()`](https://docs.snowflake.com/en/sql-reference/functions/regexp_replace), [`SEARCH()`](https://docs.snowflake.com/en/sql-reference/functions/search) and so on. But what if you need a function that isn't provided? That's where [User Defined Functions](https://docs.snowflake.com/en/developer-guide/udf/udf-overview) (UDF) come in! In Snowflake, you can write your UDFs in SQL, Python, Java, Scala, and JavaScript. There are  3 types of UDFs: scalar, tabular, and aggregate. Let's go over the different UDF types: \n",
    "\n",
    "| Type      | Input            | Output              | Description                                                                                                                                                                            |\n",
    "|-----------|------------------|---------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Scalar    | One row          | One row             | This is probably the type of UDF you think of intuitively. For every input row, this returns an output row (one in, one out).                                                          |\n",
    "| Tabular   | One row          | One or more rows    | UDTFs can return multiple rows for a single input row. Common examples include breaking up arrays or objects into multiple rows by their attributes.                                   |\n",
    "| Aggregate | One or more rows | One row (per group) | UDAFs can help you do, well, aggregations! UDAFs are similar to functions like SUM, AVG, and STDDEV: you can use them with GROUP BY clauses to calculate domain-specific aggregations. |\n",
    "\n",
    "If you've used UDFs before, you may be familiar with [Vectorized UDFs](https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-udfs#label-snowpark-python-udf-vectorized), which allow you to process **batches** of rows at a time in your scalar and tabular UDFs. When you tag a scalar UDF or UDTF with the `@vectorized` decorator, that tells Snowflake to provide batches of input rows to your UDF/UDTF in the form of [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). This can be helpful for using Python libraries that operate on pandas DataFrames, and can be more performant for some workloads. Vectorized UDFs and UDTFs are only supported for Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c907d40-4a06-4dcb-8ef9-8d0b3fe257b1",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "### Writing UDFs\n",
    "\n",
    "To create a UDF with Python, you can annotate a Python method with the [`@udf`](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.functions.udf) decorator. This will use your Snowpark Session to create the UDF. (Under the hood it will run `CREATE OR REPLACE` with the source code to create the object.) Or, you can run [`Session.udf.register()`](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.udf.UDFRegistration.register#snowflake.snowpark.udf.UDFRegistration.register) and pass in a reference to the Python method. \n",
    "\n",
    "The following two examples are equivalent:\n",
    "```python\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "@udf(is_permanent=True, stage_location='stage_name', replace=True)\n",
    "def combine(a: str, b:str) -> str:\n",
    "    return a+b\n",
    "```\n",
    "\n",
    "```python\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "session = Session.get_active_session()\n",
    "\n",
    "def combine(a: str, b:str) -> str:\n",
    "    return a+b\n",
    "\n",
    "session.udf.register(combine, is_permanent=True, stage_location='stage_name', replace=true)\n",
    "```\n",
    "\n",
    "Let's create a basic UDF and call it in a query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df71ff-1dcd-4ae5-b01b-1c634d9cf328",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "# Create a UDF\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "# TODO: Have them create a stage in the beginning setup, we'll need it here and possibly in other sections\n",
    "\n",
    "@udf(is_permanent=True, stage_location='TODO')\n",
    "def my_udf(something) -> int:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301b7a6-572e-41c0-85e0-1a44c364ae56",
   "metadata": {
    "language": "sql",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "-- Now we can call this UDF in a SQL query\n",
    "SELECT my_udf(col)\n",
    "FROM TASTY_BITES_TABLE\n",
    "WHERE ...\n",
    "LIMIT 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170bdf3-4871-4ae4-b445-a06d58fa4ba9",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "# And of course, we can use this UDF in a DataFrame query as well\n",
    "# TODO: Query with Menu item with DF.apply\n",
    "menu_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df68a4-f68a-4338-9083-cab10643eb10",
   "metadata": {
    "collapsed": false,
    "name": "UDAF_1"
   },
   "source": [
    "### Writing a UDAF\n",
    "\n",
    "Now that we're familiar with creating and calling basic UDFs, let's take this up a notch and try using a [User Defined **Aggregate** Function](https://docs.snowflake.com/en/developer-guide/snowpark/python/creating-udafs) (UDAF)! UDAFs allow us to implement custom aggregation logic or aggregations that Snowflake does not provide built-in. UDAFs are inherently more complicated than scalar UDFs -- you have have multiple input rows per group which need to be aggregated. Therefore, you will use a [Python Class](https://www.w3schools.com/python/python_classes.asp) to implement the required methods. The definition below illustrates the four methods that Snowflake needs.\n",
    "\n",
    "\n",
    "```python\n",
    "class MyUDAF:\n",
    "    def ___init___(self):\n",
    "        self._initial_value = None\n",
    "\n",
    "    def accumulate(self, input_value):\n",
    "        # This is where input rows will be passed. In this method, aggregate the input_value with the\n",
    "        # self._initial_value. (For example: summing, appending to a list, etc.)\n",
    "        self._initial_value += input_value\n",
    "    \n",
    "    @property\n",
    "    def aggregate_state(self):\n",
    "        # This exposes the self._initial_value with a known name so Snowflake can access it during execution\n",
    "        return self._initial_value\n",
    "    \n",
    "    def merge(self, other_aggregate_state):\n",
    "        # This merges two intermediate aggregates. On large datasets, Snowflake will call this to merge \n",
    "        # intermediate results. The input, other_aggregate_state, is the @property of another instance of this class\n",
    "        self._inital_value += other_aggregate_state\n",
    "    \n",
    "    def finish(self):\n",
    "        # Snowflake will call this method to retrieve the final result\n",
    "        return self._partial_sum\n",
    "```\n",
    "\n",
    "Here is a concrete example. This UDAF calculates an average.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class AvgAggState:\n",
    "    # An average is the sum of values divided by the count, so this tracks the rolling sum and counts.\n",
    "    sum: int\n",
    "    count: int\n",
    "\n",
    "class PythonAvg:\n",
    "    def __init__(self):\n",
    "        # This is the initial value (sum and count are both set to 0)\n",
    "        self._agg_state = AvgAggState(0, 0)\n",
    "\n",
    "    @property\n",
    "    def aggregate_state(self):\n",
    "        # This exposes the intermediate state under a known property name, agregate_state, so Snowflake can access it.\n",
    "        return self._agg_state\n",
    "\n",
    "    def accumulate(self, input_value):\n",
    "        # input_value will be an input row, so we add the value to the sum and increment the count\n",
    "        sum = self._agg_state.sum\n",
    "        count = self._agg_state.count\n",
    "        \n",
    "        self._agg_state.sum = sum + input_value\n",
    "        self._agg_state.count = count + 1\n",
    "\n",
    "    def merge(self, other_agg_state):\n",
    "        # other_agg_state will be an AvgAggState, so we combine the sums and counts\n",
    "        sum = self._agg_state.sum\n",
    "        count = self._agg_state.count\n",
    "        \n",
    "        other_sum = other_agg_state.sum\n",
    "        other_count = other_agg_state.count\n",
    "        \n",
    "        self._agg_state.sum = sum + other_sum\n",
    "        self._agg_state.count = count + other_count\n",
    "\n",
    "    def finish(self):\n",
    "        # Finally, return the sum divided by the count!\n",
    "        sum = self._agg_state.sum\n",
    "        count = self._agg_state.count\n",
    "        return sum / count\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593934a3-32dc-449a-8d1b-4b5d11507183",
   "metadata": {
    "language": "python",
    "name": "UDAF_2"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "from snowflake.snowpark.types import FloatType, DateType\n",
    "from snowflake.snowpark.functions import udaf\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.functions import udaf\n",
    "\n",
    "class LoyaltyScoreUDAF:\n",
    "    def __init__(self):\n",
    "        self.total_orders: int = 0\n",
    "        self.total_value: float = 0\n",
    "        self.last_order_ts: datetime = None\n",
    "\n",
    "    def accumulate(self, order_total, order_ts):\n",
    "        if order_total is not None:\n",
    "            self.total_orders += 1\n",
    "            self.total_value += order_total\n",
    "            \n",
    "        if order_ts is not None:\n",
    "            if self.last_order_ts is None or order_ts > self.last_order_ts:\n",
    "                self.last_order_ts = order_ts\n",
    "\n",
    "    def merge(self, other: tuple):\n",
    "        self.total_orders += other[0]\n",
    "        self.total_value += other[1]\n",
    "        if self.last_order_ts is None or (other[2] and other[2] > self.last_order_ts):\n",
    "            self.last_order_ts = other[2]\n",
    "\n",
    "    def finish(self):\n",
    "        if self.total_orders == 0:\n",
    "            return 0.0\n",
    "        avg_order_value = self.total_value / self.total_orders\n",
    "        \n",
    "        now = datetime.utcnow().date()\n",
    "        days_since_last = (now - self.last_order_ts).days if self.last_order_ts else 999\n",
    "        recency_boost = 1 / (days_since_last + 1)\n",
    "\n",
    "        # Weighting parameters\n",
    "        w1, w2, w3 = 1.0, 0.1, 5.0\n",
    "\n",
    "        return (\n",
    "            w1 * math.log(self.total_orders + 1) +\n",
    "            w2 * avg_order_value +\n",
    "            w3 * recency_boost\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def aggregate_state(self):\n",
    "        return (self.total_orders, self.total_value, self.last_order_ts)\n",
    "      \n",
    "loyalty_udaf = udaf(\n",
    "    LoyaltyScoreUDAF, \n",
    "    name=\"LoyaltyScoreUDAF\",\n",
    "    is_permanent=True,\n",
    "    stage_location='@my_stage',\n",
    "    replace=True, \n",
    "    return_type=FloatType(), \n",
    "    input_types=[FloatType(), DateType()])\n",
    "\n",
    "print(f'Created UDAF, \"{loyalty_udaf.name}\" ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbeb563-983b-479e-87a6-76ec2be4222e",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE TASTY_BYTES_ORDERS (\n",
    "  ORDER_ID BIGINT,\n",
    "  TRUCK_ID INT,\n",
    "  ORDER_TS TIMESTAMP_TZ,\n",
    "  ORDER_DETAIL_ID BIGINT,\n",
    "  LINE_NUMBER INT,\n",
    "  TRUCK_BRAND_NAME STRING,\n",
    "  MENU_TYPE STRING,\n",
    "  PRIMARY_CITY STRING,\n",
    "  REGION STRING,\n",
    "  COUNTRY STRING,\n",
    "  FRANCHISE_FLAG BOOLEAN,\n",
    "  FRANCHISE_ID INT,\n",
    "  FRANCHISEE_FIRST_NAME STRING,\n",
    "  FRANCHISEE_LAST_NAME STRING,\n",
    "  LOCATION_ID INT,\n",
    "  PLACEKEY STRING,\n",
    "  LOCATION_NAME STRING,\n",
    "  TOP_CATEGORY STRING,\n",
    "  SUB_CATEGORY STRING,\n",
    "  LATITUDE FLOAT,\n",
    "  LONGITUDE FLOAT,\n",
    "  CUSTOMER_ID INT,\n",
    "  FIRST_NAME STRING,\n",
    "  LAST_NAME STRING,\n",
    "  E_MAIL STRING,\n",
    "  PHONE_NUMBER STRING,\n",
    "  CHILDREN_COUNT INT,\n",
    "  GENDER STRING,\n",
    "  MARITAL_STATUS STRING,\n",
    "  MENU_ITEM_ID INT,\n",
    "  MENU_ITEM_NAME STRING,\n",
    "  QUANTITY INT,\n",
    "  UNIT_PRICE FLOAT,\n",
    "  PRICE FLOAT,\n",
    "  ORDER_AMOUNT FLOAT,\n",
    "  ORDER_TAX_AMOUNT FLOAT,\n",
    "  ORDER_DISCOUNT_AMOUNT FLOAT,\n",
    "  ORDER_TOTAL FLOAT\n",
    ");\n",
    "\n",
    "copy into jasonfreeberg.public.tasty_bytes_orders\n",
    "from @frostbytes/harmonized/orders_v.csv\n",
    "on_error = continue;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dca55-3324-4bf1-ba82-f0ca47567292",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "UDAF_x"
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  CUSTOMER_ID,\n",
    "  LoyaltyScoreUDAF(ORDER_TOTAL, ORDER_TS) AS loyalty_score\n",
    "FROM tasty_bytes_orders\n",
    "WHERE \n",
    "    ORDER_TOTAL IS NOT NULL \n",
    "    AND ORDER_TS IS NOT NULL\n",
    "    AND CUSTOMER_ID IS NOT NULL\n",
    "GROUP BY CUSTOMER_ID\n",
    "ORDER BY loyalty_score DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295c0ad-865f-496f-9e68-5bf60fd1554f",
   "metadata": {
    "collapsed": false,
    "name": "external_access_1"
   },
   "source": [
    "## Accessing External Data\n",
    "\n",
    "Next, let's leverage the flexbility of Python to enrich our Tasty Bytes orders data with weather data to understand how weather conditions impact customer behavior—do people order more tacos when it's sunny? Fewer smoothies when it's cold? \n",
    "\n",
    "To do so, we will use [OpenWeather](https://openweathermap.org/)'s [REST API](https://openweathermap.org/api/one-call-3#history) to fetch information about the weather for the time, longitude, and latitude when that order was made. OpenWeather allows you to make 1,000 API requests per day. Not familiar with REST APIs? Check out [this article](https://tutorialedge.net/software-eng/what-is-a-rest-api/), or feel free to call an instructor over for more information!\n",
    "\n",
    "First, let's create an External Access Integration to allow code in our Snowflake account to reach out to OpenWeatherMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e66846-ba39-44c1-b0ed-dcb069bb7fc3",
   "metadata": {
    "language": "sql",
    "name": "external_access_2"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE NETWORK RULE openweathermap_network_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = ('api.openweathermap.org');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01b854-03e1-4074-a75c-9db86e151c7c",
   "metadata": {
    "language": "sql",
    "name": "external_access_3"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION openweathermap_access_integration\n",
    "  ALLOWED_NETWORK_RULES = (openweathermap_network_rule)\n",
    "  ENABLED = true;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6b03c-ba0f-4f64-98ea-9065d1e6e50a",
   "metadata": {
    "collapsed": false,
    "name": "external_access_4"
   },
   "source": [
    "Now that the network rule and external access integration are created, we need to associate it with this Notebook by following [these steps](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks-external-access#enable-external-access-integrations-eai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0fd26-c417-4861-b808-1999ee25719e",
   "metadata": {
    "language": "python",
    "name": "external_access_"
   },
   "outputs": [],
   "source": [
    "# Since the Notebook was restarted, let's re-import these modules\n",
    "# re-create the session object.\n",
    "import streamlit as st\n",
    "import modin.pandas as pd\n",
    "import snowflake.snowpark.modin.plugin\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413b2fb-199f-4a56-b54f-bf1e95fdb94b",
   "metadata": {
    "language": "python",
    "name": "external_access_5"
   },
   "outputs": [],
   "source": [
    "# TODO: Put the API key in the Secrets store\n",
    "\n",
    "def get_url(lat, lon, time):\n",
    "    # This function constructs the URL that we will send requests to\n",
    "    API_KEY = '34719d209ca19d3e008f1374cd55ac63'\n",
    "    #return f'https://api.openweathermap.org/data/3.0/onecall/timemachine?lat={lat}&lon={lon}&dt={time}&appid={API_KEY}'\n",
    "    \n",
    "    # TODO: this functionaly works but it ignores the time param, it just gets current weather\n",
    "    return f'https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&dt={time}&exclude=hourly,daily&appid={API_KEY}'\n",
    "    \n",
    "# Let's try it here:\n",
    "example_url = get_url(37.7749, -122.4194, 1744596465)\n",
    "\n",
    "print(f'Example URL to send requests to...\\n{example_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f2c03-aeca-4709-bba1-f6a716fccd6d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "external_access_6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def get_weather(lat: int, long: int, time: datetime):\n",
    "    # This will get the temp, cloudiness, and description for the\n",
    "    # weather at the given latitude, longitude, and time.\n",
    "\n",
    "    epoch_time = int(time.timestamp())\n",
    "    \n",
    "    url = get_url(lat, long, epoch_time)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.ok:\n",
    "        json_ = response.json()\n",
    "\n",
    "        # Pull out the data we want\n",
    "        return {\n",
    "            'temp': json_['main']['temp'],\n",
    "            'cloudiness': json_['clouds']['all'],\n",
    "            'description': json_['weather'][0]['description']\n",
    "        }\n",
    "        \n",
    "    else:  # Bad response\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e13ab0-da13-43c0-83c9-0971a5844aee",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "external_access_7"
   },
   "outputs": [],
   "source": [
    "print(\"Let's do a test run of the function!\")\n",
    "get_weather(37.7749, 122.4194, datetime(2023, 1, 1, 12, 30, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04714273-c51c-4971-a040-10886d3bf03b",
   "metadata": {
    "language": "python",
    "name": "external_access_8"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import VariantType, StringType, LongType, TimestampTimeZone, TZ, TimestampType\n",
    "\n",
    "session.udf.register(\n",
    "    get_weather,\n",
    "    name='get_weather',\n",
    "    replace=True,\n",
    "    return_type=StringType(),\n",
    "    input_types=[StringType(), StringType(), TimestampType(TimestampTimeZone.TZ)],\n",
    "    packages=['requests'],\n",
    "    external_access_integrations=['openweathermap_access_integration']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bf168-d064-412e-90b3-f05a4d5a2eb6",
   "metadata": {
    "language": "sql",
    "name": "external_access_9"
   },
   "outputs": [],
   "source": [
    "select \n",
    "    latitude, longitude, order_ts, \n",
    "    get_weather(latitude, longitude, order_ts) as weather\n",
    "from TASTY_BYTES_ORDERS\n",
    "limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfa25f-3945-4a2b-8d13-b74b6c55a249",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": [
    "# TODO: A Python example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e87e28-5319-4f6c-b472-f32fa309a709",
   "metadata": {
    "collapsed": false,
    "name": "orchestration_1"
   },
   "source": [
    "## Scheduling Python Jobs\n",
    "\n",
    "- Introducde Tasks\n",
    "- Explain that stored procedures and Notebooks can be executed from Tasks. Briefly weigh pros and cons of these options\n",
    "- Example DDL for a Task definition\n",
    "- Show button to Schedule this Notebook\n",
    "\n",
    "In Snowflake, Tasks are used to automate and schedule jobs. Tasks can schedule the execution of a Notebook, a stored procedure, or arbitrary SQL commands. Check out the docs for [`CREATE TASK`](https://docs.snowflake.com/en/sql-reference/sql/create-task) for more information, but at its core a Task needs a **schedule** and a **warehouse**. For example, the SQL below defines a Task that runs every hour with the Warehouse, `my_wh`, and calls a stored procedure named `my_stored_procedure`.\n",
    "\n",
    "```sql\n",
    "USE DATABASE TEST_DB;\n",
    "USE SCHEMA TEST_SCHEMA;\n",
    "\n",
    "CREATE TASK my_task\n",
    "  WAREHOUSE = mywh\n",
    "  SCHEDULE = '60 MINUTES'\n",
    "  AS\n",
    "    CALL my_stored_procedure\n",
    "\n",
    "ALTER TASK my_stored_procedure RESUME;\n",
    "```\n",
    "\n",
    "And of course, you can define the same Task using Python!\n",
    "\n",
    "```python\n",
    "from datetime import timedelta\n",
    "from snowflake.core.task import Cron, Task\n",
    "\n",
    "tasks = root.databases[\"TEST_DB\"].schemas[\"TEST_SCHEMA\"].tasks\n",
    "\n",
    "task = tasks.create(\n",
    "    Task(\n",
    "        name=\"my_task\",\n",
    "        definition=\"CALL my_stored_procedure\",\n",
    "        schedule=Cron(\"0 * * * *\", \"America/Los_Angeles\"),\n",
    "        warehouse=\"my_wh\"\n",
    "    ),\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a8bbc-f5bb-4778-96de-ccac4bb8edb1",
   "metadata": {
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81dc7cee-a09e-4ab6-94db-79a7c580e98e",
   "metadata": {
    "collapsed": false,
    "name": "cell13"
   },
   "source": [
    "## Monitor and Troubleshoot Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d8567-7013-4134-9186-6e84b45ccbda",
   "metadata": {
    "collapsed": false,
    "name": "cell40"
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4976c-3500-4614-8978-ef0062524fe9",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "## Streamlit/Reporting/visualizations (Doris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "jason.freeberg@snowflake.com",
   "authorId": "126490907320",
   "authorName": "JASONFREEBERG",
   "lastEditTime": 1745028740570,
   "notebookId": "x4v3zno5sjosohxhwarx",
   "sessionId": "4bbcc9a4-8405-4535-8a3e-b4f256c62975"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
