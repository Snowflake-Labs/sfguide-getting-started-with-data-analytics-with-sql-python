{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "kgcfyee2r4nxaxi6slpd",
   "authorId": "126490907320",
   "authorName": "JASONFREEBERG",
   "authorEmail": "jason.freeberg@snowflake.com",
   "sessionId": "db457c9d-4feb-4212-8923-98fb9f58d1d1",
   "lastEditTime": 1745797861100
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32acc9a-10cd-4ab6-874c-4ababd3c91c3",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "# Part 3: Bringing Workflows to Production\n\n- Add the snowflake.core package\n- The Notebook tracing need to be enabled on the account, the proc tracing doesn't seem to work\n\n## Scheduling Python Jobs\n\nIn Snowflake, [Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro) are used to automate and schedule jobs. Tasks can schedule the execution of a Notebook, a stored procedure, or arbitrary SQL statements. In this Notebook, you will learn how to use a Task to schedule a Python Stored Procedure to apply the Customer Loyalty Score UDAF on a schedule. Next, you will learn how to set up telemetry collection for the job and set up alerts.\n\n## Creating a Python Stored Procedure\n\nIf you're not already familiar with [Stored Procedures](https://docs.snowflake.com/en/developer-guide/stored-procedure/stored-procedures-overview), they allow you to run *procedural* code, like loops, if/else blocks, and other patterns which are hard to do in SQL alone. \n\nTo create a Python Stored Procedure, all we need is a Python function that accepts a Snowpark Session object as its first argument. In Snowflake we call this the \"handler\" for the procedure. For example, the Python function below is a bare-bones procedure handler that uses the Session object which returns an integer:\n\n```python\nfrom snowflake.snowpark import Session\n\ndef my_handler(sess: Session) -> int:\n    n_rows = sess.table('my_table').count()\n    return n_rows\n```\n\nLet's create a procedure to calculate the Customer Loyalty scores and write them to an output table. Then, we will schedule it with a Task so we can have up-to-date loyalty scores. "
  },
  {
   "cell_type": "code",
   "id": "4e0324b7-0e23-42f2-9b68-5b4679142de2",
   "metadata": {
    "language": "sql",
    "name": "cell17"
   },
   "outputs": [],
   "source": "--CREATE DATABASE LAB_DB;\n--USE DATABASE LAB_DB;\nCREATE STAGE MY_STAGE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85c1943f-1386-4094-a3b8-81b5431cc746",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom datetime import datetime\nfrom snowflake.snowpark.functions import lit\nimport logging\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2cf1c890-23b0-422d-9f46-ed6d4de0ee0b",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import IntegerType\n\n# Handler for the procedure\ndef calc_loyalty_scores(sess: Session) -> int:\n    logging.info('Starting loyalty score procedure')\n    calculated_at = datetime.now()\n    \n    sess.sql(\"\"\"\n        SELECT\n          CUSTOMER_ID,\n          LoyaltyScoreUDAF(ORDER_TOTAL, ORDER_TS) AS loyalty_score\n        FROM tasty_bytes_orders\n        WHERE \n            ORDER_TOTAL IS NOT NULL \n            AND ORDER_TS IS NOT NULL\n            AND CUSTOMER_ID IS NOT NULL\n        GROUP BY CUSTOMER_ID\n        ORDER BY loyalty_score DESC\n    \"\"\").with_column(\"calculated_at\", lit(calculated_at))\\\n    .write\\\n    .save_as_table('loyalty_scores', mode='append')\n\n    # Returns the number of rows in the output table\n    return sess.table('tasty_bytes_orders').count()\n\n\n# Register the handler as a Stored Procedure\nloyalty_sproc = session.sproc.register(\n    calc_loyalty_scores,\n    return_type=IntegerType(),\n    name='calculate_loyalty_scores',\n    is_permanent=True,\n    stage_location='@my_stage/',\n    replace=True\n)\n\nprint(f'Created procedure \"{loyalty_sproc.name}\"')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00b0acbd-7065-4754-93e7-d669818fc079",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Call the stored proc for a test run\nCALL CALCULATE_LOYALTY_SCORES();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90f938b9-3d17-47fe-b846-a364815fb12a",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "Now let's create a Task to run this stored procedure on a schedule. Check out the docs for [`CREATE TASK`](https://docs.snowflake.com/en/sql-reference/sql/create-task) for more information, but at its core a Task needs a **schedule** and a **warehouse**. For example, the SQL below defines a Task that runs every hour with the Warehouse, `my_wh`, and calls a stored procedure named `my_stored_procedure`.\n\n```sql\nUSE DATABASE TEST_DB;\nUSE SCHEMA TEST_SCHEMA;\n\nCREATE TASK my_task\n  WAREHOUSE = my_wh\n  SCHEDULE = '60 MINUTES'\n  AS\n    CALL my_stored_procedure\n```\n\nAnd of course, you can define the same Task using Python!\n\n```python\nfrom datetime import timedelta\nfrom snowflake.core.task import Cron, Task\n\ntasks = root.databases[\"TEST_DB\"].schemas[\"TEST_SCHEMA\"].tasks\n\ntask = tasks.create(\n    Task(\n        name=\"my_task\",\n        definition=\"CALL my_stored_procedure\",\n        schedule=Cron(\"0 * * * *\", \"America/Los_Angeles\"),\n        warehouse=\"my_wh\"\n    ),\n)\n```"
  },
  {
   "cell_type": "code",
   "id": "1c93436e-79a0-4f0f-88c4-13e1cb09ceaf",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# Let's create a Task in Python\nfrom snowflake.core import Root\nfrom snowflake.core.task import Cron, Task\n\nroot = Root(session)\ntasks = root.databases[\"LAB_DB\"].schemas[\"PUBLIC\"].tasks\nschedule = Cron(\"*/5 * * * *\", \"America/Los_Angeles\")  # every 5 minutes\n\ntask = tasks.create(\n    Task(\n        name=\"loyalty_score\",\n        definition=\"CALL LAB_DB.PUBLIC.CALCULATE_LOYALTY_SCORES();\",\n        schedule=schedule,\n        warehouse=\"my_wh\"\n    ),\n    mode=\"orreplace\"\n)\n\n# Start the task, will run every 5 minutes now\ntask.resume()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ac7e2fd-0a09-46f7-8b18-2554fae3fad5",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "Now the Task, `loyalty_score`, should be running every 5 minutes. In the Database browser on the left, find your database and under the PUBLIC schema you should see the `loyalty_score` Task listed under the *Tasks* tab.\n\n![Task Location](https://raw.githubusercontent.com/Snowflake-Labs/sfguide-getting-started-with-data-analytics-with-sql-python/refs/heads/main/images/task_location.png?token=GHSAT0AAAAAADCOV42D5XGD3APNPIY26U2M2AFJ7JQ) \n\nClick the task and it will bring you to the page for the Task. Click the **Run History** page to see the executions of the Task."
  },
  {
   "cell_type": "code",
   "id": "5e5ac7a1-84b3-4086-9fb6-84cffdf7576a",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "# Now let's stop the Task's execution\ntask.suspend()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da1ad170-89b1-44e7-8a39-c6473725960a",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "## Monitoring Python Workloads\n\nNow we have a simple Python job scheduled to execute every 5 minutes. This raises the question: how do we monitor this job as it's running? How can we be notified if it fails overnight? How can we track performance over time? Snowflake provides a [suite of observability capabilities](https://www.snowflake.com/en/product/features/snowflake-trail/) to help you address those questions. Let's go over a few of Snowflake's observability capabilities:\n\n1. Telemetry Collection: You can route any [logs](https://docs.snowflake.com/en/developer-guide/logging-tracing/logging), [metrics](https://docs.snowflake.com/en/developer-guide/logging-tracing/metrics), or [traces](https://docs.snowflake.com/en/developer-guide/logging-tracing/tracing) from your jobs to Snowflake's Event Tables for storage and alerting.\n1. History tables: Use the [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity), [Copy History](https://docs.snowflake.com/en/user-guide/data-load-monitor), and [Task History](https://docs.snowflake.com/en/user-guide/ui-snowsight-tasks) to monitor all usage in your account.\n1. [Alerts and Notifications](https://docs.snowflake.com/en/developer-guide/builders/observability#label-observability-alerts-notifications): Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring.\n1. [Extensibility with third-party tools](https://docs.snowflake.com/en/developer-guide/builders/observability#label-observability-tools-analysis-visualization): The Snowflake [event table](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up) adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your Snowflake telemetry can easily be consumed by other ecosystem tools."
  },
  {
   "cell_type": "markdown",
   "id": "1c0d995d-a2ce-41cd-a6fa-5de1c33126d5",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "## Event Table Setup\n\nThe Event Table is the central storage system for logs, metrics, and traces in your account. A [default Event Table](https://docs.snowflake.com/developer-guide/logging-tracing/event-table-setting-up#use-the-default-event-table) is already created at `snowflake.telemetry.events`. Let's enable it:"
  },
  {
   "cell_type": "code",
   "id": "20d9b603-5d00-4e5c-bce6-0e654b3686c2",
   "metadata": {
    "language": "sql",
    "name": "cell13"
   },
   "outputs": [],
   "source": "ALTER ACCOUNT SET EVENT_TABLE = snowflake.telemetry.events;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "119bb4e8-ac78-4ac7-9fff-d79905b97891",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "Next, we'll enable the collection of logs, metrics, and traces in our account."
  },
  {
   "cell_type": "code",
   "id": "32bff270-e3e6-494e-a695-bd0a33068035",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "ALTER ACCOUNT SET LOG_LEVEL = 'INFO';\nALTER ACCOUNT SET METRIC_LEVEL = 'ALL';\nALTER ACCOUNT SET TRACE_LEVEL = 'ALWAYS';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7af20828-fe80-48cd-90fc-a79f433d72b7",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "We're now collecting logs, metrics, and traces for all jobs in our account. To test this, let's run our stored procedure in the cell below. After running this procedure, go to **Monitoring** > **Traces and Logs** > **Log Explorer**. You should see the log message \"`Starting loyalty score procedure`\" from the procedure in the **Log Explorer**. You can use the filters at the top to find the log message. Look under the Object column for the name of the procedure, \"`CALCULATE_LOYALTY_SCORES()`\".\n\n![](https://raw.githubusercontent.com/Snowflake-Labs/sfguide-getting-started-with-data-analytics-with-sql-python/refs/heads/main/images/log_explorer.png?token=GHSAT0AAAAAADCOV42CEFCF5UPR5AYAGZQ22AFO46A)\n\nClick that log line and a pane will open on the right with more information about the log entry, like the object which emitted it, the warehouse used, and the file and line where the log line was emitted from. \n\nClick the button to go the Query associated with the log.\n\n![](https://raw.githubusercontent.com/Snowflake-Labs/sfguide-getting-started-with-data-analytics-with-sql-python/refs/heads/main/images/link_to_query_page.png?token=GHSAT0AAAAAADCOV42DHY4VTYASB2E6LYZU2AFPABQ)\n\nTODO:\n1. Show trace diagram\n1. select span, show metrics"
  },
  {
   "cell_type": "markdown",
   "id": "e5eca549-d161-4984-b4ec-68b33fd63907",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "## Custom Telemetry\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "689e57c5-56d8-4817-9b51-63a65becdf4c",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "## Next Steps\n\nCongratulations! Over these three Notebooks you've learned how to query data with Snowpark pandas, write user defined functions, external access, schedule jobs with Tasks, and monitor your jobs with logs, metrics, and traces. Now that you've completed this Hands-on Lab, here are some additional resources to keep learning.\n\n### Create email notifications \n\nTry using what you learned about logs, metrics, and traces to create an alert if a job throws an error log or if a Task fails. This is a great way to stay on top of your scheduled data pipelines. To do so, use Snowflake's [alerts](http://docs.snowflake.com/en/user-guide/alerts) feature.\n\n- [`SYSTEM$SEND_EMAIL()`](https://docs.snowflake.com/en/sql-reference/stored-procedures/system_send_email)\n- [Send email notifications](https://docs.snowflake.com/en/user-guide/notifications/email-notifications)\n\n### Visualize Data in Streamlit\n\nIf you're new to Python, you can keep learning by creating data visualizations and data-powered applications with Streamlit in Snowflake! This is a great way to learn more Python and incorporate it into your day-to-day workflows as a data professional.\n\n- [Get Started with Snowpark and Streamlit](https://quickstarts.snowflake.com/guide/getting_started_with_snowpark_for_python_streamlit/#0)\n- [Getting Started with Stramlit in Snowflake](https://docs.snowflake.com/en/developer-guide/streamlit/getting-started)\n- [Streamlit in Snowflake](https://docs.snowflake.com/en/developer-guide/streamlit/about-streamlit)\n\n"
  }
 ]
}